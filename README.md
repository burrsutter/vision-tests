# VLM Tests



#### granite3.2-vision:2b

```bash
export API_KEY=none
export INFERENCE_SERVER=http://localhost:11434/v1
export MODEL_NAME=granite3.2-vision:2b-fp16
```

#### llama3.2-vision:11b

```bash
export API_KEY=none
export INFERENCE_SERVER=http://localhost:11434/v1
export MODEL_NAME=llama3.2-vision:11b
```


#### qwen2.5-vl-7b-instruct

via LM Studio on port 1234

```bash
export API_KEY=none
export INFERENCE_SERVER=http://127.0.0.1:1234/v1
export MODEL_NAME=qwen_qwen2.5-vl-7b-instruct
```




